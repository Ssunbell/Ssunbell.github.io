---
title:  "[NLP]BPE 알고리즘(토크나이저)"
date: '2022-10-04 13:59:00 +09:00'
category: [NLP, 토크나이저]
tags: [NLP, 대회, 모두의 말뭉치]
use_math: true
---

# BPE 알고리즘

## 배경

> BPE 알고리즘은 원래 1994년에 나온 압축에 사용되는 알고리즘이였으나 최근 자연어 처리에서 사용되기 시작했다.

### Subword Segmentation

> BPE(Byte Pair Encoding)는 현재 subword segmentation(단어 분리)에 사용되는 대표적인 알고리즘이다.


subword segmentation은 하나의 단어가 여러개의 subword로 이뤄져 있다고 가정하고 단어를 더 쪼개는 전처리 작업을 의미한다.


예를 들어보자. `가다`라는 단어는 `가고, 가기, 가니, 가자` 등 다양한 용법으로 사용이 가능하다. 이때, 우리는 `가다`라는 단어를 `가-`, `다`로 나눌 수 있다. 이렇게 단어를 좀더 형태소 단위, 그것보다 작은 단위로 쪼개는 작업을 subword segmentation이라 한다.


### OOV(Out Of Vocabulary)
이러한 subword를 왜 만들어줄까? 컴퓨터는 우리가 등록한 단어만 이해할 수 있는데, 이 **등록한 단어들을 포함한 단어 모음을 단어집(Vocabulary)**이라 한다. 그런데 만약 신조어같이 단어집에 포함되지 않은 단어를 발견했을 때, 인식을 하지 못하는 문제가 발생하고, 이를 <font color='OrangeRed'>OOV(Out Of Vocabulary)</font>라고 말한다. 이런 OOV 문제를 서브워드 토크나이징을 하게 되면 인식할 수 있게 만들어줄 수 있다.


예를 들어보자. `가즈아`라는 신조어가 발생했을 경우, `가즈아`라는 단어 자체는 컴퓨터가 인식하지 못한다. 그렇지만 `가-`, `즈`, `-아`라는 3개의 서브워드로 인식하게 되면 `가-`같은 경우 `가다`에서 활용되는 `가-`를 인식하여 `가즈아`라는 단어도 해당 단어와 비슷한 단어로 인식할 수 있다.


이렇게 새로운 단어가 발생했을 경우 BPE를 통해 서브워드를 만들면 한국어나 영어 등 인식률이 높아지기 때문에 우리는 BPE 알고리즘을 이용한 토크나이저를 사용한다.


## BPE 실습
실습을 진행하기 전에 [프로그래머스 압축](https://school.programmers.co.kr/learn/courses/30/lessons/17684)을 풀어보면 도움이 된다. BPE 알고리즘의 순한 맛이라고 생각하시면 된다.

> 자세한 실습은 코랩에서 진행되고, 여기서는 인사이트를 얻기 위해 간단한 설명을 진행한다.

BPE 알고리즘을 쉽게 이해하기 위해서는 다음과 같은 키워드를 기억해야 한다.

### 1. 토큰화
토큰화라는 것은 문장을 여러개의 토큰으로 쪼개는 작업을 말한다. 이때, 토큰은 다양한 기준이 있는데, `I love you`를 `I`, `love`, `you`로 쪼갤 경우 <font color='OrangeRed'>단어 기반 토큰화(Word-Based Tokenization)</font>라고 말할 수 있다. 반면에, `I`, `l`, `o`, `v`, `e`, `y`, `o`, `u`로 쪼갠하면 <font color='OrangeRed'>문자 기반 토큰화(Character-Based Tokenization)</font>라고 말한다. 서브워드 토큰화는 문자 기반 토큰화보다는 더 뭉처진 상태로 쪼개지고 단어 기반 토큰화보다는 더 잘게 쪼개진다.

### 2. N그램(N-gram)
N그램은 N개의 **연속적인 단어 나열**을 의미한다. 만약, `N = 1`일 경우 하나의 단어만 포함하고 이를 유니그램(Unigram)이라 한다. 반면에, `N = 2`일 경우는 바이그램(bigram) `N = 3`일 경우는 트라이그램(trigram)이라 부른다.


예를 들어보면, `I love myself, I love you.`라는 문장이 있을 경우, N=1일때, `I`, `love`, `myself`, `I`, `love`, `you`로, N=2일때, `I love`, `myself I`, `love you`로, N=3일때, `I love myself`, `I love you`로 나눌 수 있다.


BPE에서는 문자 기반 토큰화로 우선 쪼갠 뒤에, **유니그램 2개를 하나의 쌍(pair)으로 묶어준 뒤, 가장 많이 등장하는 유니그램 쌍을 하나의 유니그램으로 통합**해준다.

### 예시
그렇다면 이제 간단한 BPE 예시를 보자

```python
# key값은 단어, value값은 빈도수
{low : 5,  lower : 2,  newest : 6, widest : 3}
# character-based tokenization
l, o, w, e, r, n, w, s, t, i, d
```

현재 위와 같이 low는 5개, lower은 2개, newset는 6개, widest는 3개가 존재하는 상황이다. 이를 문자 기반 토크나이징한 경우 위와 같은 <font color='OrangeRed'>문자 하나</font>를 가지는 유니그램으로 표현할 수 있다.


여기서 각각의 유니그램 2개를 합쳐 하나의 쌍으로 나타내면 다음과 같다.
```
(l, o), (l, w), (l, e), (l, r), (l, n), (l, s), (l, t), (l, i), (l, d)
(o, w), (o, e), (o, r), (o, n), (o, s), (o, t), (o, i), (o, d)
(w, e), (w, r), (w, n), (w, w), (w, s), (w, t), (w, i), (w, d)
(e, r), (e, n), (e, w), (e, s), (e, t), (e, i), (e, d)
(r, n), (r, w), (r, s), (r, t), (r, i), (r, d)
(n, w), (n, s), (n, t), (n, i), (n, d)
(s, t), (s, i), (s, d)
(t, i), (t, d)
(i, d)
```

여기서 위의 경우의 수 중에서 **딕셔너리 안에 존재**하는 유니그램 쌍은 다음과 같다.

```python
('l', 'o'): 7, ('o', 'w'): 7, ('w', 'e'): 8, ('e', 'r'): 2, ('n', 'e'): 6, ('e', 'w'): 6, ('e', 's'): 9, ('s', 't'): 9, ('w', 'i'): 3, ('i', 'd'): 3, ('d', 'e'): 3
```

1. 여기서 `</w>`이 없어서 당황하는 사람도 있을텐데 원래는 있어야 하는데 간단한 이해를 위해 생략했다.
2. `</w>`가 포함되는 것은 코랩에 있으니 참고바란다.
3. 밸류값으로 정수가 있는 것을 볼 수 있는데, 이는 해당 유니그램 쌍이 등장한 횟수를 의미한다.
4. 위에서 가장 빈도수가 많은 쌍은 `('e', 's')`이다.

```
newest, widest 안에 존재하는 es는 6 * 1 + 3 * 1 = 9번 등장하여 가장 많이 등장함
```

그러므르 서브워드에 e,s를 붙여 하나의 유니그램으로 추가해준다.

```
l, o, w, e, r, n, w, s, t, i, d, es # es추가
```

이러한 작업을 내가 지정한 횟수만큼 반복하게 된다.

> 좀더 구체적인 실행 예시는 [코랩](https://colab.research.google.com/drive/1gP2GLPuPmOjREN5UQkviTOEO__ccd1WN?usp=sharing)에서 확인해보자