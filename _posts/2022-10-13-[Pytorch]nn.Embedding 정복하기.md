---
title:  "[Pytorch]nn.Embedding 정복하기"
date: '2022-10-13 16:59:00 +09:00'
category: [Pytorch, torch 기본]
tags: [네이버, 부스트캠프, 네이버부스트캠프, AI Tech, 부스트캠프 AI Tech 4기, torch, Pytorch, gather, reshape, view]
use_math: true
---
![](https://user-images.githubusercontent.com/97590480/192314341-3ac916f5-4acb-4c84-83be-8c87543701e8.png)

## torch.nn.Embedding()이란?

> 우선 [파이토치 공식문서](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#embedding)를 확인해보자.

![](/assets/img/2022-10-12/1.png)
*파이토치 Embedding 함수의 정의*

> 고정된 사전(dictionary)과 크기(size)의 임베딩(embeddings)을 저장하는 간단한 룩업 테이블(lookup table).

우선 우리가 파악하기 어려운 단어로 <font color='OrangeRed'>임베딩</font>과 <font color='OrangeRed'>룩업 테이블</font>이 있다. 우선 이 두 단어를 이해해야 해당 함수를 사용할 수 있다.

![](/assets/img/B/b4.png){: width="70%" height="70%"}
*임베딩은 또 뭐고 룩업 테이블은 또 뭐죠.. 흑*

> 워드 임베딩(Word Embedding)과 임베딩 벡터(Embedding Vector)

**워드 임베딩(Word Embedding)**이란 어떤 문장 안의 단어들을 어떤 특정한 차원으로 이루어진 공간상의 한 점 또는 한 점으로 나타내는 <font color='OrangeRed'>벡터</font>로 표현하는 과정을 말한다. 그리고 이러한 워드 임베딩 과정을 거쳐서 나온 벡터가 바로 **임베딩 벡터**이다. 좀더 시각적으로 이해해보자.

![](/assets/img/2022-10-12/2.png){: width="70%" height="70%"}
*은행 관련 임베딩 벡터*

위의 단어들을 보면 단어들을 x,y,z축을 가진 3차원의 공간 상에 위치시킨 것을 볼 수 있다. ~~정확히는 단어가 5개이므로 5차원으로 표현해야 하지만 간단한 이해를 위해 저렇게 존재한다고 하자.~~

> 이렇게 공간 상에 위치시키는 이유는 <font color='OrangeRed'>단어들간의 관계</font>를 구하기 위해서이다.

1. 예금과 적금은 다른 단어들보다 좀더 가까운 거리 상에 존재하므로 좀더 밀접한 관계가 있다고 말할 수 있다.
2. 반면에, 예금과 주택대출은 멀리 위치하고 있으므로 덜 밀접한 관계가 있다고 말할 수 있다.
3. 이런 식으로 단어간의 관계를 다양한 방식으로 정의할 수 있는데, <u>그 관계를 정의하기 위해서 먼저 우리는 단어를 공간 상의 벡터로 표현해줘야 한다.</u>

> 다양한 워드 임베딩 알고리즘이 존재하지만, 임베팅 벡터는 보통 희소 벡터(sparse vector) 혹은 밀집 벡터(dense vector)로 표현한다.

![](/assets/img/B/b8.png){: width="60%" height="60%"}
*이상해도 차근차근 알아보자*

1. 우리는 단어들의 관계를 표현하기 위해 단어를 임베딩 벡터로 표현하고 이를 워드 임베딩을 통해 만들어냈다.
2. 즉, 어떤 워드 임베딩 알고리즘을 사용하냐에 따라 단어간의 관계가 다르게 표현될 수 있다.
   1. 단순히 <font color='OrangeRed'>단어의 빈도수</font>를 이용해 단어들간의 관계를 표현한 방법으로는 BOW, Count Vector, TF-IDF 등이 있다.
   2. 반면에, <font color='OrangeRed'>추론(Prediction)</font>을 기반으로 단어들간의 관계를 표현한 방법으로는 CBOW, Skip-gram, GloVe, FastText 등이 있다.
3. 이러한 워드 임베딩을 통해 두가지 방식으로 벡터를 표현할 수 있는데, 하나는 **희소 벡터**, 다른 하나는 **밀집 벡터**이다.

```python
I -> [1, 0, 0]
love -> [0, 1, 0]
you -> [0, 0, 1]
```

   1. 희소 벡터는 표현하고자 하는 단어를 1, 나머지 단어를 0으로 이분적으로 구분하여 벡터를 구성하는 희소 표현(sparse representation)으로 만든 벡터를 의미한다(혹은 원-핫(one-hot)벡터라고도 말한다). 예를 들어, `I love you`라는 문장이 있을 경우 이 문장의 희소 벡터는 위과 같다.
   2. 하지만 이 경우에, 단어의 수가 '만개'라면 벡터의 차원 수도 '만개'가 되버리기 때문에 비효율적인 방법이 되버린다.
   3. 매우 큰 문제는 <u>두 벡터의 내적 값이 항상 0</u>이라는 것이다.
      1. 이 문제는 코사인 유사도라는 두 벡터의 내적 값을 이용해 두 단어의 유사한 정도를 나타낼 때 문제가 된다.
      2. 위의 예금, 적금 예시처럼 두 단어간의 관계를 내적 값으로 구하게 되는데, 희소 행렬은 Independent Vector이기 때문에 직교하게 되고, 그 내적값은 0이 되버린다(자세한 설명은 선형대수를 참고하자).
      3. 이로 인해, <u>단어의 관계를 나타내기 위해 워드 임베딩을 진행하였지만, 희소 행렬은 그 단어의 관계를 상실하게 된다.</u>
      4. 물론 두 단어가 독립적인 것을 표현하기 위해서라면 희소 행렬이 나쁘지는 않을 것이다. 그렇지만 이를 해결하기 위해 우리는 분산 표현으로 만든 **밀집 벡터**를 사용하게 된다.

> 밀집 벡터는 분산 표현으로 나타낸 벡터를 말한다.

![](/assets/img/2022-10-12/3.png)
*분산 표현(출처:https://soobarkbar.tistory.com/3)*

1. 분산 표현은 고차원의 희소 벡터를 저차원으로 표현하기 위한 방법이다. 우선 위의 이미지를 참고해보자.
   1. 위의 그림에서 해, 달, 구름이 각각 3개씩 있으므로, 희소 벡터로 표현하면 9차원의 벡터가 나올 것이다.
   2. 그렇지만 이를 <font color='OrangeRed'>색깔, 모양</font>이라는 2가지 속성으로 묶어주게 된다면 우리는 비슷한 색깔, 모양으로 관계를 형성할 수 있다.
   3. 이처럼 2가치 속성으로 묶어주는 것을 2차원의 벡터로 표현하는 방법이 밀집 벡터이다.
   4. 이렇게 된다면 9차원의 희소 벡터를 2차원의 벡터로 줄이는 동시에 관계를 갖는 벡터로 표현할 수 있다.
   5. `I love you`를 2차원의 밀집 벡터로 변환하면 아래와 같다.

```python
# 
I -> [0,2, 0.7]
love -> [0.5, 0.2]
you -> [0.2, 0.4]
```

![](/assets/img/B/b6.png){: width="50%" height="50%"}
*임베딩을 위해 너무나도 많은 것들을 공부했다..*

> 마지막으로 룩업 테이블(lookup table)은 가중치 행렬 연산을 하지 않고 해당 인덱스를 그대로 가져오는 것을 의미한다.

```python
W = [[0.5, 0.2],
     [-0.2, 0.8]]

x = [1, 0]

x * W  = [0.5, 0.2] # W의 1행과 동일함

```
위의 예를 보면 쉽다. x가 `[1,0]`의 희소 행렬로 표현이 될 경우에, 위처럼 단지 1행을 그대로 복사해서 오는 작업과 동일하기 때문에 이 연산을 굳이 해줄 필요가 없으므로 해당되는 열을 그대로 읽어오는(lookup) 작업으로 만는 벡터를 룩업 테이블이라 한다.

> 이제 드디어 다시 pytorch의 Embedding으로 돌아올 수 있다.

![](/assets/img/B/b2.png){: width="60%" height="60%"}
*드디어..*

다시 한번 정의를 상기시켜보자.

> 고정된 사전(dictionary)과 크기(size)의 임베딩(embeddings)을 저장하는 간단한 룩업 테이블(lookup table).

1. 고정된 사전과 크기에서 사전은 단어 사전을, 크기는 밀집 벡터에서 봤었던 **차원**을 의미한다.
   - 여기서 <font color='OrangeRed'>고정된</font>의 의미를 파악해야 하는데, 위에서 보듯이 밀집 벡터의 차원은 우리가 마음대로 정할 수 있으므로 고정된 원하는 차원으로 설정하면 된다.
   - 반면에, 고정된 사전에서 넣을 파라미터는 사전의 크기(len)을 의미하므로 len()함수를 이용해 넣어주면 된다.
2. 임베딩을 저장한다는 의미는 임베딩으로 만들어주는 워드 임베딩을 저장해준다는 의미이다. 이 함수를 통해 임베딩 벡터를 생성한다.
3. 룩업 테이블은 특히 word2vec에서 임베딩 벡터를 의미한다. 그러므로 룩업 테이블이 임베딩 벡터를 나타낸다고 봐도 큰 무리는 없다.

그렇다면 함수를 직접 실행해보자.

```python
embedding = nn.Embedding(10, 3)
input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])
embedding(input)
>>>
tensor([[[-0.0251, -1.6902,  0.7172],
         [-0.6431,  0.0748,  0.6969],
         [ 1.4970,  1.3448, -0.9685],
         [-0.3677, -2.7265, -0.1685]],

        [[ 1.4970,  1.3448, -0.9685],
         [ 0.4362, -0.4004,  0.9400],
         [-0.6431,  0.0748,  0.6969],
         [ 0.9124, -2.3616,  1.1151]]])
```

1. nn.Embedding에서 왼쪽 파라미터인 `num_embeddings`이 10인 것을 볼 수 있다. 이 숫자보다 많은 크기의 단어집이 들어갈 경우 에러가 발생한다.
2. 가장 중요한 것은 nn.Embedding에서 오른쪽 파라미터인 `embedding_dim`이다. 여기서는 3차원의 밀집 벡터로 표현되도록 설정했기 때문에 해당 벡터의 <font color='OrangeRed'>마지막</font> 차원이 3이 된것을 볼 수 있다.
   1. 입력값으로 들어간 텐서의 차원은 (2,4)이다.
   2. 이를 임베딩할 경우 (2,4,embedding_dim)이 되고, 여기서는 (2,4,3)이 된다.

3. 반면에, `embedding_dim`을 2로 설정할 경우 밑의 처럼 (2,4,2)의 결과를 볼 수 있다.

```python
embedding = nn.Embedding(10, 2)
input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])
embedding(input)
>>>
tensor([[[-0.5654, -0.3817],
         [ 0.5784, -0.4835],
         [-0.2821,  0.3439],
         [ 0.3211,  0.5890]],

        [[-0.2821,  0.3439],
         [ 0.3433,  1.2730],
         [ 0.5784, -0.4835],
         [ 0.2224,  1.1746]]])
```

> 즉, nn.Embedding은 밀집 벡터로 표현하기 위한 함수이며, 이를 위해 embedding_dim값에 유의하자.