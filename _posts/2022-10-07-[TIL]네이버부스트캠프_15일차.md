---
title:  "[TIL/네이버 부스트캠프]TIL 15일차 Transformers"
date: '2022-10-07 16:59:00 +09:00'
category: [네이버 부스트캠프 AI Tech 4기, TIL]
tags: [네이버, 부스트캠프, 네이버부스트캠프, AI Tech, 부스트캠프 AI Tech 4기]
use_math: true
---
## 배운 것들

Sequnetial Model
1. 시퀀스는 어려운 문제들이 있음
   1. Trimmed sequence
   2. Ommitted sequence
   3. permuted sequence

> Transformer

1. Transformer is the first sequence transduction model based entirely on attention
2. 재귀적인 성질이 없음 : 모델은 하나의 모델이 있음 (RNN은 모델이 세번 들어감)
3. 어떤 문장이 주어지면 그것을 영어로 바꿈(Sequence to Sequence)
4. 입력 시퀀스와 출력 시퀀스가 다를 수 있음
5. 입력 시퀀스와 출력 시퀀스의 도메인이 달라도 됨 

> The Self-Attention in both encoder and decoder is the cornerstone of Transformer

1. feed forward Neural Network(Multi layer perceptron)
2. self-attention
3. 각 단어마다 세개의 벡터로 표현
4. self-attention은 xn의 decorder를 할 때, 나머지 (n-1)개를 고려함.(종속적)
5. feed forward는 종속적이지는 않음
6. 다른 단어와의 관계성을 학습
7. 3가지 벡터를 만들어냄(query, key, value)
8. score 벡터를 만들어냄
   1. query와 key의 내적을 사용
   2. 얼마나 관계가 있는지 유사도를 구함
   3. normalize를 하게 됨(8로 나눔, 하이퍼파라미터)
9. 어떤 값을 가질지는 value 벡터를 이용함(score는 weight가 됨)
10. CNN은 인풋이 고정되어 있으면 아웃풋도 고정되어 있음
11. Transformers는 인풋이 고정되더라도 아웃풋이 달라짐
12. transformers는 n개의 단어를 한번에 처리하고 n^2의 차원을 고려해야함
13. Multi-headed attention은 어려번 하는 것임
    1.  이 경우에 입력과 출력의 차원을 맞춰주는 작업이 필요
14. positional encoding(하나의 bias)
    1.  sequencial한 정보가 위의 벡터에서는 존재하지 않음
    2.  이를 표현하기 위해 순서를 단순히 concatenate함

15. transformer는 key와 value벡터를 이용해 디코더에 전달
16. autoregressive manner(하나의 단어씩 생성)

## 멘토링
1. BERT의 CLS는 단지 문장의 표시일 뿐이지 어떤 문장에 대한 의미를 담고있지는 않음
2. transformers에 대해 중요성을 다시 상기

## 느낀점
1. Transformer 구조에 대해서 열심히 배워야겠음
