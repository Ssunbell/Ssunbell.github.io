---
title:  "[PE]포지션 임베딩 논문 정리"
date: '2023-01-23 16:59:00 +09:00'
category: [DeepLearning, PE]
tags: [Position Information, Positional Embedding, 포지션 임베딩]
use_math: true
---

![](/assets/img/B/b10.png){:width="100%" height="100%"}
*위치 임베딩 개괄*

# 위치 임베딩
위치 임베딩(Positional Embedding)이란 순서 정보를 담은 임베딩을 의미한다. 위치 임베딩은 특히 NLP에서 중요한데, 왜냐면 문장은 순서가 있는(Sequential) 정보이기 때문이다.

![](/assets/img/2023-01-23/1.png){:width="100%" height="100%"}
*RNN 구조*

이는 RNN 계열의 모델에서는 크게 중요하게 여겨지지 않았다. 왜냐하면 <font color='OrangeRed'>모델 구조 자체가 순서 정보를 알아서 고려하도록</font> 설계가 되어있기 때문이다. RNN은 하나의 입력값을 받아 하나의 출력값을 반환한다. 그리고 이 출력값을 다시 입력값으로 받아 출력하는 재귀적인 구조를 가지고 있다. 이에 따라 자연스럽게 순차적으로 입력값을 받게 되는 특징을 가진다.


하지만 이러한 RNN 구조는 고질적으로 <font color='OrangeRed'>병목현상(bottleneck effect)</font> 때문에 시간 효율적이지 못하다는 특징을 가진다. 예를 들어, "I love you" 라는 문장을 RNN 모델에 입력한다면, `I -> love -> you` 순서로 입력하고 출력할 것이다. 길이가 짧다면 시간이 오래 걸리지는 않겠지만, 길이가 매우 길어진다면 당연히 시간이 그만큼 길어질 것이다. 이러한 문제를 해결하기 위해 병렬적인 구조인 어텐션 구조(attention mechanism)가 현재는 지배적인 모델 구조로 자리잡게 됐다.

![](/assets/img/2023-01-23/2.png){:width="50%" height="50%"}
*self-attention probability*

하지만 어텐션 구조는 병렬적으로 데이터를 처리할 수 있기 때문에 <font color='OrangeRed'>순서 정보를 포함하고 있지 않다.</font> 위에서 보는 그림처럼 `hello I love you`라는 문장에서 각 단어들간의 관계를 모두 고려하게 된다. 하지만 이는 현실에서는 맞지 않은 설정이 있다. 예를 들어, "I"와 "you"의 위치가 바뀌었다고 가정하자. 그렇다면 주어가 바뀌었기 때문에 `hello I love you`와 `hello you love I`는 서로 다른 의미를 가질 것이다. 하지만, 어텐션 구조는 위의 문장이 동일하다고 생각하고 관계를 학습한다. <font color='OrangeRed'>이는 어텐션이 순서 정보를 반영하지 못하는 것을 의미한다(invariant with respect to reordering of the input)</font>.


> 어떤 task에서는 위치 임베딩이 큰 영향을 미치지 않는다고 말하기도 하지만 대체로 영향을 미친다고 여러 논문들에서 말한다.


순서 정보를 고려하지 못하기 때문에 순서 정보를 추가적으로 줘야 하는데, 현재 순서 정보를 반영시키는 다양한 방법들이 존재한다. 현재 나는 DocVQA에서 LayoutLMv2의 위치 임베딩에 대해 학습하고 있으므로 그래프를 뺀 Sequential한 데이터와 vision 데이터에 대한 위치 임베딩을 정리해보고자 한다. 아래 표는 앞으로 내가 정리할 위치 임베딩에 관한 논문 리뷰 링크를 달아놓았다. 각 포스트에서 좀더 자세히 다루도록 하겠다.

|논문 제목|요약|Reference point|링크|
|:----:|:---:|:---:|
|[Attention is all you need](https://www.google.co.kr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjRr97BwN38AhVEad4KHePXB7MQFnoECA0QAQ&url=https%3A%2F%2Farxiv.org%2Fabs%2F1706.03762&usg=AOvVaw2ceXGQohV5Kx51VSkfkG08)|sinusoidal APE(add positional enbedding)|절대거리(absolute)|[링크]()|
|테스트1|테스트2|테스트3|
|테스트1|테스트2|테스트3|