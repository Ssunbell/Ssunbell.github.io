---
title:  "[TIL/네이버 부스트캠프]TIL 9일차 Pytorch 다양한 설정들"
date: '2022-09-29 16:59:00 +09:00'
category: [네이버 부스트캠프 AI Tech 4기, TIL]
tags: [네이버, 부스트캠프, 네이버부스트캠프, AI Tech, 부스트캠프 AI Tech 4기]
use_math: true
---
##  배운 것들

### multi-GPU

> Multi-GPU 환경의 필요성

- 많은 GPU, 많은 데이터 활용

- Singe vs Multi
- GPU vs Node(system) - 컴퓨터 1대
- Single Node Single GPU 일반적인 컴퓨터 한대에 GPU 한대
- Single Node Multi GPU 4 ~ 8 개
- Multi Node Multi GPU 서버실 <= 쓰기 어려움

### 멀티 GPU를 이용해 학습하는 2가지 방법

> Model parallel

1. 모델을 2개로 짤라서 병렬적으로 돌리는 방법
2. 모델을 나누는 것은 생각보다 예전부터 썼음(alexnet)
   - 최근에는 연구 분야로 자리잡음
   - 병목 현상, 파이프라인 등의 문제를 해결하기 위해
3. 알렉스넷에서 교차되는 지점이 있는데, 이것을 GPU를 사용

모델 병렬화에서 문제되는 것들
1. 잘못 구성하면 한꺼번에 진행되는 것이 아닌 하나씩만 학습이 이뤄지는 경우가 생김

> Data parallel

1. 데이터를 2개로 짤라서 모델에 각각 돌리고 평균내는 방법
2. minibatch 수식과 유사하고 한번에 여러 GPU에서 수행
3. GPU 불균형 상태가 발생(1개만 GPU 모아서 사용) -> batch 사이즈를 감소시켜줌
4. Global Interpreter Lock 제약사항

DistributedDataParallel - 각 GPU에 있는 CPU process 생성하여 개별 GPU 할당
- 기본적으로 DataParallel로 하나 개별적으로 연산의 평균을 냄

### 하이퍼파라미터 튜닝

> Hyperparameter Tuning

1. grid vs Random
   1. grid : 값을 일정한 범위로 잘라서 순차적으로 다 해봄
   2. random : 값을 랜덤하게 줘봄
2. 최근에는 베이지안 기반 기법들이 주도(베이지안 최적화)

> Ray

- multi-node multi processing 지원 모듈
- ML/DL의 병렬 처리를 위해 개발된 모듈
- 기본적으로 현재의 분산병렬 ML/DL 모듈의 표준
- Hyperparameter Search를 위한 다양한 모듈 제공

### 파이토치에서 발생하는 문제들

> PyTorch Troubleshooting

- OOM
  - Out of Memory
  - 왜 발생했는지 알기 어려움 : iteration이 돌면서 발생하기 때문에 찾기 어려움
  - Error backtracking이 이상한데로 감
  - 메모리의 이전상황의 파악이 어려움

해결방법
- batch size를 낮추고 gpu를 청소하고 돌리기(batch size 1)
- GPUutil 사용하기
  - nvidia-smi처럼 GPU의 상태를 보여주는 모듈
  - Colab은 환경에서 GPU상태를 보여주기 편함
  - iter마다 메모리가 늘어나는지 확인
- torch.cuda.empty_cache() 써보기
  - 사용되지 않는 GPU상 cache를 정리
  - 가용 메모리를 확보
  - del과는 구분이 필요
  - reset 대신 쓰기 좋음
- 1D tensor의 경우 python 기본 객체로 변환하여 처리할 것
  - .itme 혹은 float(iterloss)를 사용
- del 명령어 적절히 사용
  - forloop에서도 이미 변수가 메모리를 차지하고 있으므로 해당 변수 제거
- no_grad()사용

CUDNN_STATUS_NOT_INIT 쿠다 



device-side-assert도 OOM의 일종

colab에서 너무 큰 사이즈는 실행하지 말것

## 느낀점
- 오늘 배운 내용들은 사실 모델을 만들때 적용이 가능하므로 프로젝트에서 한번 꼭 적용해봐야겠음
- 베이지안 최적화에 대해 공부해야겠음