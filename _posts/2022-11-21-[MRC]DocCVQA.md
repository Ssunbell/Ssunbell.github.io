---
title:  "[MRC]Document Collection Visual Qustion Answering 논문 리뷰"
date: '2022-12-05 13:59:00 +09:00'
category: [논문, Moosic하게 Reading Comprehension]
tags: [NLP, 대회, RE, NER]
use_math: true
---

![](/assets/img/B/b0.png)

# 무식하게 논문읽기(MRC) DocCVQA 편🔥

## [초록](https://arxiv.org/pdf/2104.14336.pdf)
현재까지의 문서 이해에 대한 task 그리고 방법론은 문서 전체를 하나의 요소로 여기고 처리하는 것에 초점이 맞춰져 있었다. 하지만 문서들은 보통 문서의 해석에 도움을 주는 문맥 정보 (역사적인 기록, 송장 등) 뭉치(collections)로 구성되어 있다. 이 문제를 다루기 위해 우리는 문서 뭉치 시각적 질의응답(DocCVQA)에 대한 새로운 데이터셋과 관련된 데이터들을 공개하는데, 이것들은 문서 이미지의 전체 뭉치에 걸쳐서 질문이 있고, 목표는 질문에 대한 정답을 제공하는 것 뿐만 아니라 정답을 추론하는 데 필요한 정보들을 가지고 있는 문서들의 하위집합을 검색하는 것이다. 데이터셋과 함께 우리는 새로운 평가 메트릭과 새로운 데이터셋과 task에 더 많은 인사이트를 제공할 수 있는 베이스라인을 제공한다.

> Abstract: CurrenttasksandmethodsinDocumentUnderstandingaims to process documents as single elements. However, documents are usually organized in collections (historical records, purchase invoices), that pro- vide context useful for their interpretation. To address this problem, we introduce Document Collection Visual Question Answering (DocCVQA) a new dataset and related task, where questions are posed over a whole collection of document images and the goal is not only to provide the answer to the given question, but also to retrieve the set of documents that contain the information needed to infer the answer. Along with the dataset we propose a new evaluation metric and baselines which provide further insights to the new dataset and task.

## 1. 소개
문서라는 것은 시대를 지나면서 지식과 정보를 저장해주기 때문에 인류에게 있어서 필수적인 요소였다. 이런 이유 때문에, 문서에 대한 기계 이해(machine understading)을 향상시키는 데 많은 노력을 기울였다. 문서 분석과 인식(Document Analysis and Recognition; DAR)의 연구 분야에서는 초기에 인간의 이해에 초점을 맞춘 종이에 제시된 정보의 자동 추출을 목표로 한다. 가장 널리 알려진 DAR의 응용 프로그램은 텍스트를 인식하고 표 및 형태 레이아웃, 수학적 표현 그리고 그림 및 그래픽같은 시작적인 정보를 인식하여 사무용 문서를 처리하는 것을 포함합니다. 그러나, 비록 이 모든 연구 분야에서 지난 수십 년 동안 엄청나게 발전했음에도 불구하고, 실용적으로 사용할 수 있는 마지막 목표에 대해서는 지금까지 닿지 않는 이상으로 여겨지고 있었다. 게다가, 문서 뭉치들이 문서 자체만큼이나 오래되었음에도 불구하고 이 범위의 분야는 지금까지 단어 발견에서 어휘 콘텐츠에 의한 문서 검색에만 제한되어 뭉치로부터 의미론을 맹목적으로 인식하고 더 높은 수준의 정보를 추출하는 것을 무시해왔다.


반면에, 과거 몇년동안 Visual Question Answering(VQA)는 비전과 자연어 사이의 연결지점으로서 관련된 주요 Task중에 하나였습니다. 비록 [6]과 [31]이 정답에 대답하기 위해 이미지에서 텍스트를 읽는 방법론을 요구함으로써 VQA에서 텍스트를 다루기 시작했지만, 그들은 자연어 분야의 문제로 좁게 생각했다. [24]는 문서에 처음으로 VQA를 도입한 사람이었습니다. 하지만, 이전의 연구들은 실제 영역과 문서 모두 뭉치 이미지 관점을 고려하지 않았다. 


이와 관련하여 우리는 더 나은 문서 뭉치 이해와 단어 찾어 찾기를 넘어서기 위해 Document Collection Visual Qustion Answering(DocCVQA)를 제시한다. DocCVQA의 목적은 질문을 하게 되면 문서 이미지의 뭉치에서 정보를 뽑아내고 정답을 제공하기 위한 방법을 예상하는 것이다. **그러나 이러한 정답들이 필수적인 정보를 포함한 문서들을 사용하여 추론되었는지 확인하기 위해 방법론은 또한 정답의 증거로써 confidence list의 형태로 정답을 얻는데 사용되는 그 문서의 ID를 제공해야 한다.** 그러므로, 우리는 이 task를 검색-응답 task로 설계하며, 이 방법론은 반드시 초기에 다른 데이터셋으로 룬련되어야 하며, 결과적으로 우리는 이 문서 뭉치에 오직 20개의 질문만 던지게 된다. 추가적으로, 그림 1의 질문 예제에서 관찰할 수 있듯이, 이 과제의 대부분의 대답은 실제로 순서와 관련이 없는 다른 문서에서 추출된 단어 집합이다 따라서, 우리는 이 작업에서 방법의 응답 성능을 평가하기 위해 평균 정규화된 레벤슈테인 유사성(ANLS)을 기반으로 새로운 평가 메트릭을 정의한다. 마지막으로, 우리는 이 작업과 데이터 세트에 대한 통찰력을 제공하는 매우 다른 관점에서 두 가지 기준 방법을 제안한다.


데이터셋, 베이스라인 코드와 온라인 평가 서비스와 함께 성능 평가 방식은 여기서 확인할 수 있다(https://docvqa.org).


![](/assets/img/2022-12-05/figure1.png)
*그림 1 상단 : DocCVQA에서 예시 문서의 일부 사진. ID가 454인 왼쪽 문서는 아래 질문의 답변과 관련있는 문서 중 하나이다. 하단 : 예시 데이터에서 예시 질문, 정답과 증거(id). DocCVQA에서 증거는 추론할 수 있는 정답이 있는 문서. 이 예시에서 올바른 정답은 2016과 2022, 그리고 증거들은 미 상원의 후보자로써 Anna M. Rivers가 출마하는 등록 문서에 해당하는 id가 454, 10901인 문서 이미지*


## 2. 선행 연구

### 2.1 문서 이해(Document understanding)

문서 이해는 문서로부터 관련된 정보를 자동적으로 추출하는 마지막 목표를 가진 문서 분석과 관련된 커뮤니티 내에서 지금까지 많이 조사되었다. 대부분의 연구들은 양식, 송장, 영수증, 여권 또는 신분증, 이메일, 계약서 등과 같은 구조화되거나 반구조화된 문서에 초점이 맞춰졌다. 이전 연구들[9, 29]은 각 새로운 유형의 문서에 대한 특정 템플릿을 정의하는데 필요한 대전제 규칙들을 기반으로 연구가 되었다. 나중에는 학습 기반의 방법론[8, 26]을 이용해 자동적으로 문서의 종류를 분류하고 사전에 학습된 템플릿 없이 관련된 분서 분야를 식별할 수 있었다. 최근의 딥러닝의 발전으로[20, 36, 37] 자연어처리, 비전 특징 추출, 그래프 기반 표현을 기반을 활용하여 정보추출 과정에서 단어 의미와 시각적인 레이아웃을 고려하는 더 전체적인(global) 시각의 문서를 다룬다.


이러한 모든 방법은 주로 상향식 접근법에 따라 문서 특징부터 관련된 의미론적 정보까지 키-밸류 쌍을 추출하는 것에 초점을 맞춘 방법이다. 본 연구에서는 반대로 시각적 질문 응답 페러다임을 사용해서 하향식 접근법을 사용하며, 여기서 목표는 문서의 정보 검색을 주도한다.


### 2.2 문서 검색(Document retrieval)
많은 문서들의 뭉치에서 관련된 정보를 검색하는 도구를 제공하는 것은 지금까지 문서 검색에 초점이 맞춰져 있었다. 지금까지 대부분의 연구들은 단어 찾기[27] 관점, 즉 명시적으로 노이즈가 많은 OCR에 의존하지 않고 문서 뭉치 안에서 특정 쿼리가 될 수 있는 단어들을 찾는 관점에서 이 task를 다루었다. 현재(2021.06)의 단어 검색에서 SOTA 모델은 심층 네트워크[17, 32]를 사용해서 쿼리 문자열과 단어 이미지를 투영시킨 공통의 임베딩에서 유사도 검색 기반의 모델이다. 전체 뭉치을 검색하기 위해서는 이러한 표현들은 문서 안에서 주어진 문장에 관한 모든 인스턴스(instances)를 찾기 위해 물체 인식(object detection)과 관련된 표준적인 딥러닝 아키텍쳐를 결합한다.


**단어 찾기는 해당 단어가 있는 문맥의 의미적인 맥락을 고려하지 않고 뭉치 안의 주어진 단어가 있는 특정 인스턴스들을 찾는 검색만 실행한다. 이와는 반대로 본 task에서 제안하는 과제는 특정 의미론적으로 고립된 단어를 찾는 것이 아닌 쿼리 질문에 기반해서 의미론적인 검색을 하는 것을 목표로 한다.**

### 2.3 시각적 질의-응답(Visual Question Answering)
시각적 질의 응답(VQA)는 이미지와 해당 이미지의 자연어 질문이 주어지고 정확한 자연어 대답을 얻어내는 것이 목표인 task이다. 초창기에 [21, 28]과 [3]에서 제안되었으며, 이 task를 위한 대규모 데이터 셋을 제안했다. 이러한 데이터셋의 이미지들은 모두 실제 사진으로 구성되어 있고, 질문들은 주로 이미지에 존재하는 물체들을 가리킨다.


그럼에도 불구하고, 이 분야는 매우 인기를 끌었고, 실제 사진에서 텍스트를 고려한 첫번째 데이터셋인 ST-VQA[6] 및 TextVQA[31]와 같은 새로운 문제를 탐구하는 몇 가지 새로운 데이터 셋이 출시되었다. ST-VQA에서 정답은 항상 **이미지에서 발견된 텍스트 내에 포함**되어 있는 반면에, TextVQA는 텍스트를 읽어야 하지만 답은 **이미지에서 인식된 텍스트에 존재하지 않을** 수 있다. VQA에서 텍스트를 통합하는 것은 두가지 주요 과제들을 제시한다. 첫번째로, 가능한 답변의 클래스 수가 기하급수적으로 증가했고, 답변 또는 인풋 데이터인 인식된 텍스트에서 단어집에 없는 단어를 인식하지 못하는(Out of Vocabulary; OOV) 문제가 발생했다. OOV 문제를 해결하기 위해, Fasttext[7], PHOC[1]와 같은 임베딩 기법이 더 대중화되고, 답변을 에측하기 위해 가장 빈번한 답변들을 고정하는 표준 단어집과 함께 복사 메커니즘(copy mechanism)은 답변으로써 OCR 토큰을 제안[31]할 수 있었다. 나중에 [13]은 분류 출력을 고정된 단어집 혹은 각 시점(timestep)에서 인식된 텍스트로부터 산출된 단어인 디코더로 변경했고, 복잡하고 긴 답변에서 더 많은 유연성을 제공했다.


문서와 관련해서 FigureQA[16]과 DVQA[15]는 다른 종류의 차트와 그래프같은 복잡한 figure와 데이터 표현에 초점을 맞췄는데, 데이터셋을 합성하거나 이러한 구조물에 질의 응답을 상응시키는 작업을 진행했다. 더 최근에는 [23]이 문서 이미지에 대한 첫 번째 VQA 데이터 세트인 DocVQA를 제안했다. 여기서 질문은 그림, 양식 또는 표뿐만 아니라 복잡한 레이아웃의 텍스트도 참조한다. 데이터 셋과 함께 NLP와 사진 텍스트 VQA 모델에 기반한 baseline을 제안한다. 이 분야에서, 우리는 문서 뭉치에 대한 연구를 한단계 더 확장한다.


마지막으로 이 논문과 가장 유사한 연구는 ISVQA[4]이며 여기서 질문은 동일한 장면을 다른 관점으로 구성된 작은 이미지 세트를 통해 구성된다. 설정이 비슷해 보이지만, 결코 데이터 셋을 해결하는 방식이 동일하지 않다는 것을 유의해야 한다. ISVQA의 경우 모든 이미지는 동일한 문맥을 공유하며, 이는 이미지 중 하나에서 일부 정보를 찾는 것이 다른 이미지 셋의 정보를 찾는데 도움이 될 수 있다는 것이다. 또한, 이미지 셋들은 항상 6개의 이미지로 구성된 작은 이미지 집합이며, 반대로 DocCVQA의 전체 뭉치들에서 이미지는 텍스트를 고려하지 않은 실사 이미지라는 것이다. 예를 들어, ISVQA에서 제안하는 베이스라인은 HME-VideoQA[11]과 모든 이미지를 이어 연결하는 표준 VQA 방법이지만 이는 우리의 문제에 적합하지 않은 방식이다.

## 3. DocCVQA 데이터셋
이번 장에서는 이미지를 모으고 질의 응답을 모은 과정을 설명하고, 모은 데이터에 대한 설명과 이번 task에서 평가하는 메트릭에 대해서 설명하겠다.

### 3.1 데이터 수집(Data Collection)

![](/assets/img/2022-12-05/figure1.png)
*그림 1*

- Image

DocCVQA 데이터셋은 정치 켐페인의 자금 조달, 로비스트 지출, 공공 기관과 정치인들의 재정 문제에 대해 공공으로 접근이 가능한 정보를 제공하는 것을 목표로 하는 기관인 Public Disclosure Commision(PDC)에서 공공 데이터 포털에서 받아온 14,362개의 문서 이미지로 구성되어 있다. 우리는 여러가지 이유로 이 출처로부터 문서들을 받았다. 첫째, 주기적으로 업데이트되는 라이브 저장소이므로 연구에 필요하거나 유익하다고 판단될 경우 향후 데이터 세트의 크기를 늘릴 수 있습니다. 또한 레이아웃과 내용 면에서 의미가 있고 전체 뭉치에 대해 추론하는 데 흥미로울 수 있는 문서 유형이 포함되어 있다. 또한 문서들을 CSV 파일 형식으로 제공하여 비용이 많이 드는 주석 프로세스 없이 일련의 질문을 제기하고 답변을 얻을 수 있다. 원본 문서 이미지 모음에서 기록들이 부분적으로 누락되거나 모호한 다중 페이지 문서 및 문서를 모두 폐기했습니다. 따라서 데이터 세트에 최종적으로 포함된 모든 문서는 동일한 문서 템플릿인 미국 후보 등록 양식에서 제공되었으며 시간 경과에 따른 변경으로 인해 약간의 구조 차이가 있었다. 그러나 제안된 방법은 손으로 쓴 텍스트와 컴퓨터로 쓴 텍스트뿐만 아니라 복잡한 레이아웃을 동시에 이해해야 하기 때문에 이러한 문서는 여전히 몇 가지 과제를 제기한다. 우리는 그림 1에 몇 가지 문서 예제를 제공한다.

![](/assets/img/2022-12-05/figure2.png)

- Qustion and Answers

DocCVQA 데이터셋이 검색-응답 task에 맞춰 설정되고 문서들은 비교적 유사하다는 점을 고려하여 이 뭉치에 대해서 20개의 자연어 질문만 제시한다. 질문과 답변을 수집하기 위해 우리는 먼저 복잡성(숫자, 날짜, 후보자 이름, 확인란 및 다른 양식 필드 레이아웃)과 가변성 측면을 고려하여 문서 양식에서 가장 중요한 필드가 무엇인지 분석했다(섹션 3.2 참조). 또한 특정 값을 찾기 위해 질문을 몇가지 유형으로 제한하는 것은 단순한 단어 찾기 시나리오로 빠질 수 있기 때문에 질문의 유형을 위해 종류에 대한 제약 조건을 정의했다. 날짜의 경우, 질문은 특정 날짜 이전, 이후 및 사이 또는 특정 연도의 문서를 참조한다. 다른 텍스트 필드의 경우 질문은 특정 값(P 정당 후보)이 있는 문서, 특정 값(P 정당 후보)이 없는 문서 또는 가능한 집합(P, Q 또는 R 정당 후보)의 값을 포함하는 문서를 참조합니다. 체크 박스의 경우 값이 선택되었는지 여부에 대한 제약 조건을 정의했습니다. 마지막으로, 필드와 우리가 정의한 제약사항에 다라 자연어로 된 질문들은 전체 뭉치을 참조하고, 문서 자체 대신에 특정 값들을 요청하도록 되어있다. 우리는 표 1의 테스트 세트에 있는 전체 질문 목록을 제공한다.


![](/assets/img/2022-12-05/figure3.png)

일단 질문을 받으면 우리는 이미지와 함께 다운로드된 주석으로부터 정답을 얻을 수 있다. 그런 다음, 우리는 몇몇 데이터들의 주석이 잘못되었음을 발견하고 답변이 올바르고 모호하지 않도록 수동으로 확인했다. 마지막으로, 우리는 질문을 8개의 질문이 있는 샘플 데이터와 나머지 12개의 테스트 셋 두가지로 데이터를 분할했다. 문서 레이아웃의 낮은 가변성을 고려하여, 우리는 테스트 세트에서 문서 양식 필드를 참조하거나 샘플 세트에서 볼 수 없는 일부 제약 조건이 있는 질문이 있는지 확인했다. 또한 그림 2와 같이 관련 문서의 수는 질문에 따라 상당히 다양하며, 이는 향후 다른 방식으로 다뤄야할 또다른 과제이다.


### 3.2 통계량과 분석(Statistics and Analysis)

우리는 질문과 예상 답변을 수행하는 데 사용되는 문서 양식 필드에 대한 간략한 설명과 주석의 값 수와 고유한 값을 보여주는 가변성에 대한 간략한 분석을 표 2에서 제공한다.

![](/assets/img/2022-12-05/figure4.png)

### 3.3 평가 메트릭(Evaluation metrics)

본 연구의 궁극적인 목표는 문서 모음에서 정보를 추출하는 것이다. 그러나, 이전에 증면된바와 같이 특히 분균형한 데이터 셋에서 모델은 특정 질문에 대한 특정 답변이 오히려 일반적이라는 것을 학습할 수 있다. 가장 명확한 경우 중 하나는 예 또는 아니오로 대답한 질문에 대한 것이다. 이를 방지하기 위해 질문에 대한 답변 뿐만 아니라 질문에 대한 대답을 평가할 뿐만 아니라 우리가 증거물로 생각할 수 있고 질문에 대한 대답의 정보를 포함하고 있는 문서로부터 답변을 추론했는지를 평가한다. 따라서, 우리는 검색 성능에 관련된 증거물에 관련된 것과 VQA 성능에 기반한 정답에 관련된 평가 두가지를 고려한다.


- Evidences

표준 검색 Task[19]에 따르면 우리는 평균 정밀도(MAP)를 사용하여 positive 증거물들을 올바르게 맞췄는지 평가하는 기법을 사용한다. 우리는 질문에 대한 답을 찾을 수 잇는 문서를 positive 증거물로 간주한다.