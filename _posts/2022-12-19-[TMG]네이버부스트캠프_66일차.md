---
title:  "[TMG/네이버 부스트캠프]TIL 66일차 MRC 강의 듣기"
date: '2022-12-19 16:59:00 +09:00'
category: [네이버 부스트캠프 AI Tech 4기, TIL]
tags: [네이버, 부스트캠프, 네이버부스트캠프, AI Tech, 부스트캠프 AI Tech 4기]
use_math: true
---

## 오늘의 목표
- MRC 강의 6강까지 듣기 -> 완료

## 배운 것들

### 1강 Introduce MRC

#### MRC의 종류

1. Extractive Answer Datasets

- 질의에 대한 답이 항상 주어진 지문의 segrment or span으로 존재
    - Close Tests는 빠진 단어를 맞춤, 지문 안에 답이 있어야 함.
    - QA는 Close Tests처럼 비슷하지만 빠진 단어가 아닌 질문이 들어감.

2. Descriptive/Narrative Answer Datasets
    - 답이 지문 내에서 추출한 span이 아니라, 질의를 보고 생성된 sentence

3. Multiple-choice Datasets
    - 질의에 대한 답을 여러 개의 answer candidates 중 하나로 고르는 형태


#### Challenges in MRC

1. 단어들의 구성이 유사하지는 않지만 동일한 의미의 문장을 이해
    - 질문의 단어가 들어있지 않은 문장
    - 문단 전체를 이해해야 답변할 수 있는 질문

2. Unanswerable questions
    - 단어가 있다고 해서 정답이 아닐 수 있기 때문에 정답이 아니라면 반환하지 말아야 함.

3. Multi-hop reasoning
    - 답변을 내기 위해서는 다른 Document를 참조해야만 답변을 낼 수 있음.

#### MRC 평가 방법

- Exact Match : Accuracy
- F1 score : 예측한 답과 ground-truth 사이의 token overlap을 F1으로 계산
- ROUGE-L Score : 예측한 값과 ground-truth 사이의 overlap recall을 n-gram으로 계산
- BLEU : 예측한 답과 ground-truth 사이의 precision을 n-gram으로 계산

> Unicode
전 세계의 모든 문자를 일관되게 표현하고 다룰 수 있도록 만들어진 문자셋, 각 문자마다 숫자 하나에 매핑한다.

- 한국어는 한자 다음으로 유니코드에서 가장 많은 코드를 차지
    - 완성형 : 현대 한국어의 자모 조합으로 나타낼 수 있는 모든 완성형 한글 11,172자
    - 조합형 : 조합하여 글자를 만들 수 있는 초, 중, 종성

- ex) 가 : U+AC00 (U+는 유니코드를 의미, AC00은 16진수를 의미)

> 인코딩 & UTF-8

- 인코딩이란 문자를 컴퓨터에서 저장 및 처리할 수 있게 이진수로 바꾸는 것
- UTF-8
    - 현재 가장 많이 쓰는 인코딩 방식
    - 문자 타입에 따라 다른 길이의 바이트를 할당
        - 3 bytes : 한글을 포함한 대부분의 글자
        - 4 bytes : 이모지 포함
- 파이썬의 string 타입은 유니코드 표준을 사용
- ord()는 string을 유니코드 code point로 변환
- chr()은 code point를 문자로 변환

> Tokenization

- 텍스트를 토큰 단위로 나누는 것(단어, 형태소, subword 등)
- BPE 방식 : 가장 자주 나오는 글자 단위 Bigram을 다른 글자로 치환

> KorQuAD 훑어보기

- 인공지능이 한국어 질문에 대한 답변을 하도록 필요한 학습 데이터셋
- 1550개의 위키피디아 문서에 대해서 10649건의 하위 문서들과 크아루드 소싱을 통해 제작한 63952개의 질의응답 쌍으로 구성
- 누구나 데이터를 내려받고 학습한 모델을 제출하고 공개된 리더보드에 평가를 받을 수 있음
- 2.0은 1.0보다 길이가 더 길고 표와 리스트 등을 포함

1. 데이터 수집 과정
    - 대상 문서 수집 : 문단 단위로 정제, 이미지/표 URL 제거
    - 짧은 문단, 수식이 포함된 문단 제거
    - 크라우드 소싱을 통해 질의응답 70000+ 쌍 생성
    - 작업자가 양질의 질의응답 쌍을 생성하도록 상세한 가이드라인을 제공함
    - 앞에서 생성한 질문에 대해 사람이 직접 답해보면서 Human performance 측정
    - 앞서 질문/답변 생성 과정에 참여한 사람은 참여 불가

2. HuggingFace datasets 라이브러리 소개
    - 접근 가능한 모든 데이터셋이 memory-mapped, cached 되어있어 데이터를 로드 하면서 생기는 메모리 공간 부족이나 전처리 과정 반복의 번거로움 등을 피할 수 있음

### Extraction-based MRC

> Extraction-based MRC 문제 정의

- 질문(question)의 답변(answer)이 항상 주어진 지문(context) 내에 span으로 존재 e.g. SQuAD, KorQuAD, NewsQA, Natural Qustions, etc.

- F1 score를 기준으로 평가
    - Precision : 예측값이 길어질게 되면 값이 내려감
    - Recall : 둘이 겹치는 단어의 갯수가 너무 적으면 값이 내려감

- input은 질문 + 문장 순으로 들어감
    - input_ids를 이용해서 질문과 문장이 다른 문장임을 표시
- 모델 출력값
    - 정답은 문서 내 존재하는 연속된 단어토큰(span)이므로, span의 시작과 끝 위치를 알면 정답을 맞출 수 있음
    - Extraction-based에선 답안을 생성하기 보다, 시작 위치와 끝 위치를 예측하도록 학습함. 즉, Token Classification 문제로 치환
    - 만약에 문장이 부분적으로 나눠진다면(ex : 육군참모총장 -> (진)육군참모(구)총장) 어쩔수 없이 그 시작과 끝 인덱스를 반환

> Post-processing(후처리)

- 불가능한 답 제거하기
    - End Position이 start position보다 앞에 있는 경우(e.g. start=90, end=80)
    - 예측한 위치가 context를 벗어난 경우(e.g. qustion 위치쪽에 답이 나온 경우)
    - 미리 설정한 max_answer_length보다 길이가 더 긴 경우
- 최적의 답안 찾기
    - Start/end position prediction에서 score (logits)가 가장 높은 N개를 각각 찾는다.
    - 불가능한 start/end 조합을 제거한다.
    - 가능한 조합들을 score의 합이 큰 순서대로 정렬
    - Score가 가장 큰 조합을 최종 예측으로 선정
    - Top-k가 필요한 경우 차례대로 내보낸다.

### Generation-based MRC

> Generation-based MRC 문제 정의

- Extraction-baesd mrc : 지문 내 답의 위치를 예측 -> 분류 문제, Negative log likelihood function
- Generation-based mrc : 주어진 지문과 질의를 보고 답변을 생성 -> 생성 문제, teacheer forcing

- token_type_ids가 들어가지 않음

### Passage Retrieval
> 질문에 맞는 문서를 찾는 것

- Open-domain Qustion Answering : 대규모의 문서 중에서 질문에 대한 답을 찾기
- Passage Retrieval과 MRC를 이어서 2-stage로 만듦
- Query와 Passage를 임베딩한 뒤 유사도로 랭킹을 매기고, 유사도가 가장 높은 Passage를 선택함.

Passage Embedding Space
- Passage Embedding의 벡터 공간
- 벡터화된 Passage를 이용하여 Passage간 유사도 등을 알고리즘으로 계산할 수 있음

>  Sparse Embedding 소개
- BoW를 구하는 방법 : n-gram(unigram, bigram)
- Term value를 결정하는 방법 : term이 document에 등장하는지 혹은 얼마나 많이 등장하는지

> Sparse Embedding 특징
1. Dimension of embedding vector = number of terms
    - 등장하는 단어가 많아질수록 증가
    - N-gram의 n이 커질수록 증가
2. Term overlap을 정확하게 잡아 내야 할 때 유용
3. 반면, 의미(semantic)가 비슷하지만 다른 단어인 경우 비교가 불가

> TF-IDF
- Term Frequency(TF) : 단어의 등장빈도
    1. Raw count
    2. Adjusted for doc length : raw count / num words(TF)
    3. Other variants : binary, log normalization, etc.
- Inverse Document Frequency : 단어가 제공하는 정보의 양
    - 100개 문서 모두에 the가 들어가기 때문에 DF는 100이 됨.
    - 모든 문서에 들어있는 단어의 경우 IDF의 값이 0이 되고, 반면에 한 문서에만 등장할 경우 DF는 1이 되서 IDF값이 매우 높아짐

### Dense Embedding
- sparse vector의 한계점
    - 차원의 수가 매우 크다. -> compressed format으로 극복 가능
    - 유사성을 고려하지 못한다.

- Dense Embedding이란?
Complementary to sparse representations by design
- 더 작은 차원의 고밀도 벡터 (length = 50 - 1000)
- 각 차원이 특정 term에 대응되지 않음
- 대부분의 요소가 non-zero값






## 마스터 클래스
- 서비스 배포라는 것은 기한 내에 완수해야만 하는 역할
- 반대로 회사와 멀어지면서 연구를 하게 되면 리소스에 대한 부족함을 느낌

MRC 수업 설계의도
- NLP가 아닌 MRC라는 목적성을 가지고 학습
- 과학보다 제품
- Do not reinvent the wheel(코드를 가져와서 사용)
- 문장 안에서 정보를 어떻게 하면 잘 추론할 수 있는지에 대한 연구를 진행중
- 네이버에서 중요하게 생각하는 것은 기본기, 문제 있는 것이 없냐, 모든 분야에서 평타 이상을 치느냐, 불확실성을 안고 시작 따라서 직관이 좀더 필요