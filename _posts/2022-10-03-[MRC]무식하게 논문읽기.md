---
title:  "[MRC]XLM-RoBERTa 논문 리뷰"
date: '2022-10-03 13:59:00 +09:00'
category: [논문, Moosic하게 Reading Comprehension]
tags: [NLP, 대회, 모두의 말뭉치]
use_math: true
---

![](/assets/img/B/b0.png)

# 무식하게 논문읽기(MRC) XLM-RoBERTa 편🔥

## XLM-RoBERTa

> 배경

'2022 국립국어원 인공 지능 언어 능력 평가'를 참가하는 중이다. 으레 그렇듯 대회에서 제공하는 baseline 모델을 살펴보려 하는데, 내가 잘 모르는 XLM-RoBERTa 사전 학습 모델을 사용하였기에 이에 대한 이해를 위해 MRC를 해보려 한다.

### 사전지식

> XLM 이라는 모델을 RoBERTa에 적용한 것

1. RoBERTa 모델은 2019년 7월에 발표한 BERT 모델을 최적화한 모델이다.
2. XLM은 **Cross-lingual Language Model Pretraining**을 의미하며, 교차언어 모델이라는 의미에 맞게 다국어 학습을 목표로 하는 사전학습 모델이다.
   1. 다국어 모델인 만큼 단일 언어 및 병렬 데이터셋으로 학습을 진행한다.
   2. XLM은 <font color='OrangeRed'>인과 언어 모델링(CLM), 마스크 언어 모델링(MLM), 번역 언어 모델링(TLM)</font>이라는 세가지 중요 학습 방법을 사용한다. 반면에, BERT는 MLM만 학습한다.
   3. **CLM(causal language modeling)**이란 이전 토큰들을 기반으로 다음 토큰을 예측하는 작업을 의미하며, 모델은 훈련과정에서 확률 p를 모델링한다.
   4. **TLM(translation language modeling)**이란 병렬 말뭉치에 존재하는 원본문장과 타겟문장을 하나의 입력으로 연결한 후, MLM과 같이 문장 일부를 `<MASK>`로 치환하고 이를 원본문장으로 복원한다.
3. XLM-R(XLM-RoBERTa)는 비지도 기반 XLM을 적용한 모델이다.
   1. RoBERTa를 사용하는 것은 Meta에서 논문을 냈기 때문이다.
   2. 2020년 8월에 발표했다.
   3. 다국어 분류 문제, sequence 라벨링, QA에서 좋은 성능을 보인다.
   4. 약 100개의 언어를 적용하였음에도 성능을 올릴 수 있었다고 한다.

## 초록(Abstract)

> This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks.

이 논문은 대규모의 양방향 언어 사전학습 모델이 다양한 다국어 번역 문제에 대해 상당한 성능 상향을 보이고 있다는 것을 보여줍니다.

> We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data.

2TB 이상의 필터링된 CommonCrawl 데이터를 사용하여 100개 언어에서 Transformer 기반 마스크 언어 모델을 학습했습니다.

>  Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER.

XLM-R이라고 하는 우리 모델은 XNLI에서 +14.6%의 평균 정확도, MLQA에서 +13%의 평균 F1 점수, NER에서 +2.4%의 F1 점수를 포함하여 다국어 모델에서 벤치마크한 다국어 BERT(mBERT)를 크게 능가합니다.

> XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models.

XLM-R은 자원이 적은 언어에서 특히 우수한 성능을 보이며 이전 XLM 모델에 비해 스와힐리어(동아프리카쪽 언어)의 경우 XNLI 정확도가 15.7%, 우르두어(파키스탄어)의 경우 11.4% 향상되었습니다.

>  We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale.

우리는 또한 (1) 긍정효과 전이학습과 capacity dilution간의 상충관계, (2) 대규모 데이터를 가진 언어와 그렇지 않은 언어의 성능 간의 상충관계를 포함하여 이러한 이득을 달성하는 데 필요한 핵심 요소에 대한 상세한 실증 분석을 제시합니다.

> 위 내용과 관하여 자세한 내용은 [여기]()를 참고하기 바란다.

> Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.

마지막으로 언어별 성능을 희생하지 않으면서 다국어 모델링의 가능성을 처음으로 보여줍니다. XLM-R은 GLUE 및 XNLI를 벤치마크한 강력한 다국어 모델들에 비해 매우 경쟁력 있는 모델입니다. 우리는 코드, 데이터 및 모델을 공개적으로 사용할 수 있도록 할 것입니다.

## 성능 개선 방향
위의 초록처럼 다국어 저주와 용량 희석 상충관계에 대한 문제는 어쩔 수 없지만 어느정도 개선할 수 있었다고 한다. 문제를 완화하는 방법으로

1. 데이터 크기 증가
2. 파라미터 수(Capacity; 용량) 증가

라는 해결책으로 극복했다. 논문의 주장은 다음과 같다.

1. CommonCrawl 데이터를 사용하여 성능 개선을 얻을 수 있었다.
2. 차원의 저주 문제는 용량을 증가시킬수록 완화시킬 수 있었다.
3. 용량이 커짐에 따라 vocab 사이즈를 증가시킬경우 성능이 개선되었다.
4. batch_size를 크게 잡을수록 성능이 개선되었다.

## 토크나이저
XLM-R에서는 Sentence Piece Model을 사용하였다.

## 아키텍처

