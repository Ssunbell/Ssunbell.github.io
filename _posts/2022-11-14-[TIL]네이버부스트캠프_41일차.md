---
title:  "[TIL/네이버 부스트캠프]TIL 41일차 자연어 처리 소개"
date: '2022-11-14 16:59:00 +09:00'
category: [네이버 부스트캠프 AI Tech 4기, TIL]
tags: [네이버, 부스트캠프, 네이버부스트캠프, AI Tech, 부스트캠프 AI Tech 4기]
use_math: true
---
## 배운 것들
### 인공지능의 발전 과정
- 인공지능의 아이디어는 컴퓨터의 개발에 맞춰서 황금기를 이룸
- 튜링 테스트를 통해 이미테이션 게임으로 컴퓨터가 인간처럼 사고하는지 판단
- 1956 - 1974년대에 AI의 황금기 : 심리 상담사의 역할을 하도록 설계

자연어처리의 응용 분야
- 분서 분류
- 문법, 오타 교정
- 정보 추출
- 음성 인식 결과 보정
- 음성 합성 텍스트 보정
- 정보 검색
- 요약문 생성
- 기계번역
- 질의 응답
- 기계독해
- 챗봇
- 형태소 분석
- 개체명 분석
- 구문 분석
- 감성 분석
- 관계추출
- 의도 파악
- 이미지 캡셔닝
- 텍스트 압축
- 페러프레이징
- 주요 키워드 추출
- 빈칸 맞추기
- 발음기호 변환
- 소설 생성
- 텍스트 기반 게임
- 오픈 도메인 QA
- 가설 검증

대화의 단계
1. Encoder는 벡터 형태로 자연어를 인코딩
2. 메세지의 전송
3. Decoder는 벡터를 자연어로 인코딩

특징 추출과 분류
- 분류 대상의 특징(Feature)를 기준으로, 분류 대상을 그래프 위에 표현 가능
- 분류 대상들의 경계를 수학적으로 나눌 수 있음(Classficitation)
- 새로운 데이터 역시 특징을 기준으로 그래프에 표현하면 어떤 그룹과 유사한지 파악 가능
- 이러한 특징을 컴퓨터가 스스로 찾고(Feature Extraction), 스스로 분류(Classification)하는 것이 '기계학습'의 핵심

Word2Vec
- 중심 단어의 주변 단어를 이용해 중심 단어의 의미를 학습
- 그러나 OOV 문제에 치명적

FastText
- 한국어는 다양한 용언 형태를 가짐
- 용언 형태를 독립적으로 Vocab에 저장하고 subword에 집중한 word2vec 알고리즘
- 주변 단어와 중심 단어의 관계를 형성할 때 n-gram의 개념을 사용 -> 등장횟수가 적은 단에어 강점

> Word2Vec이나 FastText와 같은 word embedding 방식은 주변 문맥을 반영하지 못함. 따라서 언어 모델이 발전하게 됨

### 언어모델
- '자연어'의 법칙을 컴퓨터로 모사한 모델 -> 언어 '모델'
- 주어진 단어들로부터 그 다음에 등장한 단어의 확률을 예측하는 방식으로 학습
  
> Markov 기반의 언어 모델

- 초기의 언어 모델은 다음의 단어나 문장이 나올 확률을 통계와 단어의 n-gram을 기반으로 계산
- 해당 확률을 최대로 하도록 네트워크 학습

transformer network는 Seq2Seq처럼 두개의 인코더와 디코더가 구별되어있는 것이 아닌 인코더와 디코더를 하나의 네트워크로 구성함

### GLUE 데이터셋
- MNLI : Multi-Genre Natural Language Inference, 두 문장의 관계 분류를 위한 데이터셋
- QQP : Quora Question Paris, 두 질문이 의미상 같은지 다른지 분류를 위한 데이터셋
- QNLI : Qustion Natural Language Inference, 질의응답 데이터셋
- SST-2 : The Stanford Sentiment Treebank, 영화 리뷰 문장에 관한 감성 분석을 위한 데이터셋
- CoLA : The Corpus of Linguistic Acceptability, 문법적으로 맞는 문장인지 틀린 문장인지 분류를 위한 데이터셋
- STS-B : The Semantic Textual Similarity Benchmark, 뉴스 헤드라인과 사람이 만든 paraphrasing 문장이 의미상 같은 문장인지 비교를 위한 데이터셋
- MRPC : Microsoft Research Paraphrase Corpus, 뉴스의 내용과 사람이 만든 문장이 의미상 같은 문장인지 비교를 위한 데이터셋
- RTE : Recognizing Textual Entailment, MNLI와 유사하나 상대적으로 훨씬 적은 학습 데이터셋
- WNLI : Winograd NLI, 문장 분류 데이터셋
- SQuAD v1.1 : 질의응답 데이터셋
- CoNLL 2003 : Named Entity Recognition datasets, 개체명 분류 데이터셋
- SWAG : Situations With Adversarial Generations, 현재 문장 다음에 이어질 자연스러운 문장을 선택하기 위한 데이터셋

### 각 데이터셋의 특이한 점
- 두 문장 관계 분류 : 너무 상관없는 데이터로 학습한 경우 다른 일반화된 TASK에 적용하기 힘듦
- 기계 독해(KorQuAD) : 어절 단위로 EM을 평가할 경우 wordpiece tokenizer는 `이순신은` 을 `이순신 ##은`로 분절시킬 수 있는 능력이 없음. 그러나 음절 단위로 할 경우 성능이 올라감
- ETRI KoBERT : 형태소로 나눈 후 토크나이저를 사용할 경우 BERT보다 성능이 올라감
- SKT의 KoBERT : 형태소를 사용하지 않아서 ETRI KoBERT보다 성능이 떨어짐

![](/assets/img/2022-11-14/1.png)
*토크나이징에 따른 성능 비교*

- BERT의 마지막에 Entity Embedding을 추가할 경우 NER에서 좋은 성능을 보임