---
title:  "[TIL/네이버 부스트캠프]TIL 21일차 Attention"
date: '2022-10-17 16:59:00 +09:00'
category: [네이버 부스트캠프 AI Tech 4기, TIL]
tags: [네이버, 부스트캠프, 네이버부스트캠프, AI Tech, 부스트캠프 AI Tech 4기]
use_math: true
---
## 배운 것들

1. Attention은 과거의 인풋 값들을 그대로 가져오는 메커니즘을 의미
   1. 과거의 인풋 값 중에서 가중치를 줘서 가져오기 때문에 'Attention'이라는 말이 사용됨
   2. 가중치를 부여할 때, 현재의 hidden_state 값과 과거의 인풋 값의 hidden state 값의 **내적**을 통해 계산함
   3. 이를 통해 LSTM의 고질적인 문제인 병목 현상과 더 긴 문장에서도 잘 학습되는 결과를 가져옴
2. 번역에서 사용하는 accuracy는 BLEU를 사용하는데, 이는 BEAM search를 이용해서 구함.
   1. beam search는 대충 k개의 범위 안에 있는 결합 확률을 계산

## 느낀점
1. 왜 Attention이 ALL you need 인지 깨달음. attention은 모든 알고리즘에서 사용될 것 같음
2. attention을 어떻게 하면 금융 시계열 데이터에 잘 적용할 수 있을지 알아봐야겠음