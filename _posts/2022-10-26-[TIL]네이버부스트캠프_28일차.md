---
title:  "[TIL/네이버 부스트캠프]TIL 28일차 대회 3일차"
date: '2022-10-26 16:59:00 +09:00'
category: [네이버 부스트캠프 AI Tech 4기, TIL]
tags: [네이버, 부스트캠프, 네이버부스트캠프, AI Tech, 부스트캠프 AI Tech 4기]
use_math: true
---
## 배운 것들
1. val set과 test set을 굳이 나누지 않음?
2. GPU가 금방 차므로 GPU 효율적인 방식에 대한 필요성

마스터 클래스
1. Large-Scale 모델은 다양한 문제에 적용이 가능하기 때문에 재사용성이 좋음

Generative : 노이즈를 주고 다시 복원하는 작업
- DAE : 오리지널이 아닌 노이즈를 주고 복원을 하려면 encoder가 오히려 본질에 집중하게 됨(self-supervised learning의 핵심)
    - BART : text에 mask, delete를 해주고 다시 채워주는 작업을 통해 모델의 이해도를 높임

Contrastive : 인풋을 두개 줌
- Siamese Network : 두개의 이미지 안에 같은 돼지를 주고 두 이미지를 비교함
- 이미지를 잘라서 다양하게 섞고 다시 복원함
- BERT : 다음 문장인지 아닌지 예측하는 NSP Task

Generative-Contrastive
- 데이터에 손상을 주고 복원한 뒤, 비교하는 Task
- ELECTRA : 문장 안에 MASK를 주고 생성한 뒤, Descriminator로 비교를 수행함

![](/assets/img/2022-10-26/1.png)
*supervised learning 계통도*

2. supervised learning과 self-supervised learning을 결합할 때 정확도가 크게 개선됨

## 느낀 것들
1. GAN이 대세가 될거 같음