---
title:  "[TIL/네이버 부스트캠프]TIL 23일차 self-Attention"
date: '2022-10-19 16:59:00 +09:00'
category: [네이버 부스트캠프 AI Tech 4기, TIL]
tags: [네이버, 부스트캠프, 네이버부스트캠프, AI Tech, 부스트캠프 AI Tech 4기]
use_math: true
---
## 배운 것들

- 기존의 Attention구조에서는 과거의 정보인 인코딩에서의 입력값을 가져왔지만, self-Attention은 인코딩과 디코딩을 나누지 않고 현재의 입력값을 전부 Attention구조를 사용해서 반영한다는 의미에서 self-Attention이라 부름
- 이 경우 자기 자신과의 유사도가 높다는 문제가 발생하기 때문에, 유사도를 기반으로 한 탐색 기준은 가져가되, 자기 자신과의 유사도를 낮추기 위해 query, key, value라는 세 기준을 도입해서 자기 자신과의 유사도를 낮춤
- 이러한 self-Attention이 여러개 쌓인 것들이 Multi-head Attention임.

## 느낀점
1. self-attention 구조를 만든 사람은 천재인거 같음