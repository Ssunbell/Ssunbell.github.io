---
title:  "[MRC]무식하게 논문읽기 PromCSE"
date: '2022-10-30 16:59:00 +09:00'
category: [논문, Moosic하게 Reading Comprehension]
tags: [NLP, STS, 유사도]
use_math: true
published: true
---

# Abstract
- 비교대조 학습(Contrastive learning)은 사전학습 모델(Pretrained Model; PLM)의 성능 항샹시키는 효과적인 방법입니다. 그렇지만, 비교대조 학습은 2가지 한계점을 가지고 있습니다. 첫번째, 사전 학습을 하는 단계에서 전문영역(domain)의 변화가 있는 상황에서는 낮은 성능을 얻을 수 있으므로 실제로 문장에 적용하는데 방해가 될 수 있습니다. 우리는 이 낮은 성능은 사전학습의 과도한 파라미터 갯수에 의해 발생한다고 생각했습니다. 이를 완화하고자 사전학습 모델은 고정시킨 채 오직 작은 규모의 Soft Prompt(학습 가능한 벡터들)를 훈련시키는 PromCSE를 제안합니다. 두번째로, 일반적으로 사용되는 비교대조 학습의 NT-Xent 손실 함수롤 지도학습 셋팅에서 hard negatives를 완전히 사용하지 않습니다. 우리는 NT-Xent 손실과 에너지 기반의 학습 패러다임 사이의 연결성에 영감을 받아 에너지 기반 힌지 손실을 통합하여 쌍별 차별력을 향상시킬 것을 제안합니다. 7가지 표준 의미론적 텍스트 유사성(STS) 작업과 도메인별 STS 작업에 대한 경험적 결과는 모두 현재 SOTA 문장 임베딩 모델과 비교하여 우리 방법의 효과를 보여줍니다.